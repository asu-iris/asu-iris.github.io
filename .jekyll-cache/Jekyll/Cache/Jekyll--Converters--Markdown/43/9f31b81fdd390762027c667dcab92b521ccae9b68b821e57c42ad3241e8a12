I"Ë:<p>This is <strong>Intelligent Robotics and Interactive Systems (IRIS)</strong> Lab! Our research focuses include</p>

<ul>
  <li>
    <p><strong>Contact-rich manipulation:</strong> We develop efficient physics-based representations/modeling, planning/control methods to enable robots to gain dexterity through frequently making or breaking contacts with objects.</p>
  </li>
  <li>
    <p><strong>Human-autonomy alignment:</strong>  We develop certifiable, efficient, and empowering methods to enable robots to align their autonomy with human users through various natural interactions.</p>
  </li>
  <li>
    <p><strong>Fundamental computational methods:</strong> We develop fundamental algorithms for  efficient, safe, and robust robot intelligence, by harnessing the complementary benefits of model-based and data-driven approaches.</p>
  </li>
</ul>

<p style="margin-bottom:1.2cm; margin-left: 1.5cm"> </p>

<center>
    <a href="mailto:wanxin.jin@asu.edu" target="_blank"> 
    <img src="assets/img/email_logo.png" width="40" target="_blank" /> </a>   &nbsp;&nbsp;&nbsp;
<a href="https://scholar.google.com/citations?user=SoEC4h4AAAAJ&amp;hl=en" target="_blank"> 
    <img src="assets/img/scholar_logo.png" width="40" target="_blank" /></a>   &nbsp;&nbsp;&nbsp;
<a href="https://github.com/asu-iris" target="_blank">
    <img src="assets/img/github_logo.png" width="40" target="_blank" /></a> &nbsp;&nbsp;&nbsp;
<a href="https://twitter.com/jinwanxin" target="_blank">
    <img src="assets/img/twitter_logo.png" width="40" target="_blank" /></a>  &nbsp;&nbsp;&nbsp;
<a href="https://www.youtube.com/@robotics-iris-lab" target="_blank">
    <img src="assets/img/youtube_logo.png" width="40" target="_blank" /></a>  &nbsp;&nbsp;&nbsp;

</center>

<p><br /></p>

<h3 id="recent-updates"><strong>Recent Updates</strong></h3>

<p style="margin-bottom:0.6cm"> </p>

<div class="update-item">
    <div class="update-date">Aug 24, 2024</div>
    <div class="update-content">
      <!-- <a  href="https://x.com/jinwanxin/status/1828130385806651428" target="_blank" role="button"> <i class="fa-brands fa-twitter fa-shake"></i> ğ•-Twitter </a> -->
      <p>
        ğŸš€ Tracking unknown, highly dynamic objects in contact-rich scenes â€” falling, bouncing, in-hand manipulation? Occlusion, blur, and impacts make it tough. Introducing ğŸ”¥<a href="https://arxiv.org/abs/2408.09612" target="_blank">TwinTrack</a>ğŸ”¥: a physics-aware, real-time (&gt;20â€¯Hz) tracker that fuses Real2Sim + Sim2Real, bridging vision and contact physics for reliable tracking. All GPU-accelerated.
      </p>
      <div class="video-container">
        <video autoplay="" loop="" muted="" controls="">
          <source src="/collections/research/manipulation/contactsdf/contactSDF-media.mp4" type="video/mp4" />
        </video>
      </div>
      <br />
        Check out the <a href="https://yangwen-1102.github.io/contactsdf.github.io/" target="_blank"> webpage</a>,  <a href="https://arxiv.org/abs/2408.09612" target="_blank">preprint</a>, and <a href="https://github.com/asu-iris/ContactSDF" target="_blank">code</a>.
      <br /> <br /> 

    </div>
  </div>

<div class="update-item">
    <div class="update-date">May 1, 2025</div>
      <div class="update-content">
      <p>
         ãŠ—ï¸ ğŸ‰: Our paper <a href="https://arxiv.org/abs/2502.02921" target="_blank">â€œRobust Reward Alignment via Hypothesis Space Batch Cuttingâ€</a> has been accepted to International Conference on Machine Learning (ICML) 2025. Congrats to <a href="https://zhi-xian-xie.github.io/" target="_blank">Zhixian Xie,</a>  Haode Zhang, and Yizhe Feng!
      </p> 
        Demo, webiste, and twitter feeds will arrive soon!
      </div>
  </div>
<hr />

<div class="update-item">
    <div class="update-date">Apr. 11, 2025</div>
      <div class="update-content">
      <p>
         ğŸ“¢: Wanxin will give a talk at RSS 2025 Workshop on Human-Robot Contact and Manipulation (HRCM 2025): <a href="https://hrcm-workshop.github.io/2025/" target="_blank">https://hrcm-workshop.github.io/2025/</a>. 
      </p> 
      </div>
  </div>
<hr />

<div class="update-item">
    <div class="update-date">Apr. 11, 2025</div>
      <div class="update-content">
      <p>
         ãŠ—ï¸ ğŸ‰: Our paper <a href="https://arxiv.org/pdf/2408.07855" target="_blank">â€œComplementarity-Free Multi-Contact Modeling and Optimization for Dexterous Manipulationâ€</a> has been accepted to Robotics: Science and Systems (RSS) 2025. Congrats!
      </p> 
        See our previous  
        <a href="https://x.com/jinwanxin/status/1825958958382854247" target="_blank" role="button"> <i class="fa-brands fa-twitter fa-shake"></i> ğ•-Twitter </a> and 
        <a href="https://www.youtube.com/watch?v=NsL4hbSXvFg" target="_blank" role="button"> <i class="fa-brands fa-youtube fa-shake"></i> YouTube </a> for an extended demo.
      </div>
  </div>
<hr />

<div class="update-item">
    <div class="update-date">Mar. 20, 2025</div>
      <div class="update-content">
      <p>
         ğŸ“¢: Wanxin gave a talk at Penn State: <b>Toward Efficient &amp; Real-time Robotic Dexterity:
         A model-based approach for contact-rich control, learning, &amp; perception</b>. Talk slides will be released soon!
      </p> 
      </div>
  </div>
<hr />

<div class="update-item">
    <div class="update-date">Feb. 16, 2025</div>
      <div class="update-content">
      <p>
         ãŠ—ï¸ ğŸ‰: Wen's paper <a href="https://arxiv.org/abs/2408.09612" target="_blank">â€œContactSDF: Signed Distance Functions as Multi-Contact Models for Dexterous Manipulationâ€</a> has been accepted to RA-L. Congrats to Wen!
      </p> 
        See our previous  
        <a href="https://x.com/jinwanxin/status/1828130385806651428" target="_blank" role="button"> <i class="fa-brands fa-twitter fa-shake"></i> ğ•-Twitter </a> and 
        <a href="https://youtu.be/2AsMYCT-jQI?si=yPK3kec3lj-85kts" target="_blank" role="button"> <i class="fa-brands fa-youtube fa-shake"></i> YouTube </a> for a brief introduction.
        <!-- <div class="video-container">
          <video autoplay loop muted controls>
            <source src="/collections/research/manipulation/contactsdf/contactSDF-media.mp4" type="video/mp4">
          </video>
        </div>
        <br>
          Check out the <a href="https://yangwen-1102.github.io/contactsdf.github.io/" target="_blank"> webpage</a>,  <a href="https://arxiv.org/abs/2408.09612" target="_blank">preprint</a>, and <a href="https://github.com/asu-iris/ContactSDF" target="_blank">code</a>. Here is a long demo:
        <br> <br> 
        <div class="video-container">
          <iframe src="https://www.youtube.com/embed/2AsMYCT-jQI?si=_7Y9LgnSfuF8yIoL" title="YouTube video player" frameborder="0" allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture; web-share" referrerpolicy="strict-origin-when-cross-origin" allowfullscreen></iframe>
        </div> -->

      </div>
  </div>
<hr />

<div class="updates-list">


  <div class="update-item">
    <div class="update-date">Oct 15, 2024</div>
    <div class="update-content">
      <a href="https://x.com/adcock_brett/status/1848032460023468508" target="_blank" role="button"> <i class="fa-brands fa-twitter fa-shake"></i> ğ•-Twitter </a>
      <p>
      ğŸ”¥ğŸ”¥ <strong>â€œSkills from YouTube, No Prep!â€</strong> ğŸ”¥ğŸ”¥
      Can robots learn skills from YouTube without complex video processing?
      Our <a href="https://arxiv.org/abs/2410.09286" target="_blank">"Language-Model-Driven Bi-level Methodâ€</a>  makes it possible! By chaining VLM &amp; LLM in a bi-level framework, we use the â€œchain ruleâ€ to guide reward learning directly from video demos. ğŸš€Check out our RL agents mastering skills from their biological counterparts!ğŸš€
      </p>
      <div class="video-container">
        <video autoplay="" loop="" muted="" controls="">
          <source src="/collections/research/human/lfd-llm/lfd-llm.mp4" type="video/mp4" />
        </video>
      </div>
      <br />
        Check out the <a href="https://arxiv.org/abs/2410.09286" target="_blank">preprint</a>. Here is a long demo:
      <br /> <br /> 
      <div class="video-container">
        <iframe width="560" height="315" src="https://www.youtube.com/embed/CzlyYLu4mLQ?si=jyh1nZdADkGYpAAQ" title="YouTube video player" frameborder="0" allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture; web-share" referrerpolicy="strict-origin-when-cross-origin" allowfullscreen=""></iframe>
      </div>
    </div>
  </div>



<br /><br /><br />




  <div class="update-item">
    <div class="update-date">Aug 24, 2024</div>
    <div class="update-content">
      <a href="https://x.com/jinwanxin/status/1828130385806651428" target="_blank" role="button"> <i class="fa-brands fa-twitter fa-shake"></i> ğ•-Twitter </a>
      <p>
        ğŸš€ Can a robotic hand master dexterous manipulation  <strong>in just 2 minutes</strong>? YES! ğŸ‰ Excited to share our recent work  <a href="https://arxiv.org/abs/2408.09612" target="_blank">â€œContactSDFâ€</a>, a physics-inspired representation using signed distance functions (SDFs) for contact-rich manipulation, from geometry to MPC. ğŸ”¥ Watch <strong>a full, uncut video</strong> of Allegro hand learning from scratch below!   We are pushing the boundaries of â€œfastâ€ learning and planning in dexterous manipulation.
      </p>
      <div class="video-container">
        <video autoplay="" loop="" muted="" controls="">
          <source src="/collections/research/manipulation/contactsdf/contactSDF-media.mp4" type="video/mp4" />
        </video>
      </div>
      <br />
        Check out the <a href="https://yangwen-1102.github.io/contactsdf.github.io/" target="_blank"> webpage</a>,  <a href="https://arxiv.org/abs/2408.09612" target="_blank">preprint</a>, and <a href="https://github.com/asu-iris/ContactSDF" target="_blank">code</a>. Here is a long demo:
      <br /> <br /> 
      <div class="video-container">
        <iframe src="https://www.youtube.com/embed/2AsMYCT-jQI?si=_7Y9LgnSfuF8yIoL" title="YouTube video player" frameborder="0" allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture; web-share" referrerpolicy="strict-origin-when-cross-origin" allowfullscreen=""></iframe>
      </div>
    </div>
  </div>



<br /><br /><br />

  <div class="update-item">
    <div class="update-date">Aug 19, 2024</div>
    <div class="update-content">
      <a href="https://x.com/jinwanxin/status/1825958958382854247" target="_blank" role="button"> <i class="fa-brands fa-twitter fa-shake"></i> ğ•-Twitter </a>
      <p>
      Can model-based planning and control rival or even surpass reinforcement learning in challenging dexterous manipulation tasks? Our answer is a resounding YES! PROUD to share: ğŸ”¥ğŸ”¥<a href="https://arxiv.org/abs/2408.07855" target="_blank">"Complementarity-Free Multi-Contact Modeling and Optimization,"</a>,  our latest method that sets shattering benchmarks in various challenging dexterous manipulation tasks.
      </p>
      <p> <a href="https://arxiv.org/abs/2408.07855" target="_blank">"Complementarity-Free Multi-Contact Modeling and Optimization,"</a>  consistently achieves state-of-the-art results across different challenging dexterous manipulation tasks, including fingertip 3D in-air manipulation, TriFinger in-hand manipulation, and Allegro hand on-palm manipulation, all with different objects. Check out the demo below!
      </p>
      Our method sets a new benchmark in dexterous manipulation:
      <ul>
        <li>ğŸ¯ A 96.5% success rate across all tasks</li>
        <li>âš™ï¸ High manipulation accuracy: 11Â° reorientation error &amp; 7.8 mm position error</li>
        <li>ğŸš€ Model predictive control running at 50-100 Hz for all tasks</li>
      </ul>
      <div class="video-container">
        <video autoplay="" loop="" muted="" controls="">
          <source src="/collections/research/manipulation/teaser-grid.mp4" type="video/mp4" />
        </video>
      </div>
      <br />
        Check out our <a href="https://arxiv.org/abs/2408.07855" target="_blank"> preprint</a>, and try out our  <a href="https://github.com/asu-iris/Complementarity-Free-Dexterous-Manipulation" target="_blank">code</a> (fun guaranteed). Here is a long demo:
      <br /> <br /> 
      <div class="video-container">
        <iframe src="https://www.youtube.com/embed/NsL4hbSXvFg?si=eICS9JW-ZxMTxOtm" title="YouTube video player" frameborder="0" allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture; web-share" referrerpolicy="strict-origin-when-cross-origin" allowfullscreen=""></iframe>
      </div>
    </div>
  </div>



<br /><br /><br />
  <div class="update-item">
    <div class="update-date">July 9 2024</div>
    <div class="update-content">
      <a href="https://x.com/jinwanxin/status/1815216477748019473" target="_blank" role="button"> <i class="fa-brands fa-twitter fa-shake"></i> ğ•-Twitter </a>
      <p>
        ğŸ¤– Robots may be good at inferring a task reward from human feedback, but how about inferring safety boundaries from human feedback? In many cases such as robot feeding and liquid pouring, specifying  user-comfortable safety constraints is more challenging  than  rewards. Our recent work, led by my PhD student <a href="https://zhi-xian-xie.github.io/" target="_blank">Zhixian Xie</a>,  shows that this is possible, and can actually be very human-effort efficient! Our method is called <a href="https://arxiv.org/abs/2407.04216" target="_blank">Safe MPC Alignment</a> (submitted to T-RO), enabling a robot to learn its control safety constraints with only a small handful of human online corrections!
      </p>
      Importantly, the Safe MPC Alignment is certifiable: providing an upper bound on the total number of human feedback in the case of successful learning of safety constraints, or declaring the misspecification of the hypothesis space, i.e., the true implicit safety constraint cannot be found within the specified hypothesis space. 
      <div class="video-container">
        <video autoplay="" loop="" muted="" controls="">
          <source src="/collections/research/human/media-robot.mp4" type="video/mp4" />
        </video>
      </div>
      <br />
      Check out the <a href="https://zhi-xian-xie.github.io/safe_alignment_site/" target="_blank">project website</a>, <a href="https://arxiv.org/abs/2407.04216" target="_blank"> preprint</a>, and a breaf introduction vide below.
      <br /> 
      <div class="video-container">
        <iframe src="https://www.youtube.com/embed/QOODShHLQJE?si=IuYvkp3wm507Dc2Y" title="YouTube video player" frameborder="0" allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture; web-share" referrerpolicy="strict-origin-when-cross-origin" allowfullscreen=""></iframe>
      </div>
    </div>
  </div>

  <!-- Add more update items as needed -->
</div>

:ET