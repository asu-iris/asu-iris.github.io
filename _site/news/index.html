<!DOCTYPE html>
<html>

  <head>
    <meta charset="utf-8">
<meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">
<meta http-equiv="X-UA-Compatible" content="IE=edge">

<title>

    IRIS Lab


  | news

</title>
<!-- <meta name="description" content=""> -->

<!-- Open Graph -->


<!-- Bootstrap & MDB -->
<link href="https://stackpath.bootstrapcdn.com/bootstrap/4.5.2/css/bootstrap.min.css" rel="stylesheet" integrity="sha512-MoRNloxbStBcD8z3M/2BmnT+rg4IsMxPkXaGh2zD6LGNNFE80W3onsAhRcMAMrSoyWL9xD7Ert0men7vR8LUZg==" crossorigin="anonymous">
<link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/mdbootstrap/4.19.1/css/mdb.min.css" integrity="sha512-RO38pBRxYH3SoOprtPTD86JFOclM51/XTIdEPh5j8sj4tp8jmQIx26twG52UaLi//hQldfrh7e51WzP9wuP32Q==" crossorigin="anonymous" />

<!-- Fonts & Icons -->
<link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.14.0/css/all.min.css"  integrity="sha512-1PKOgIY59xJ8Co8+NE6FZ+LOAZKjy+KY8iq0G4B3CyeY6wYHN3yt9PW0XpSriVlkMXe40PTKnXrLnZ9+fkDaog==" crossorigin="anonymous">
<link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/academicons/1.9.0/css/academicons.min.css" integrity="sha512-W4yqoT1+8NLkinBLBZko+dFB2ZbHsYLDdr50VElllRcNt2Q4/GSs6u71UHKxB7S6JEMCp5Ve4xjh3eGQl/HRvg==" crossorigin="anonymous">
<link rel="stylesheet" type="text/css" href="https://fonts.googleapis.com/css?family=Roboto:300,400,500,700|Roboto+Slab:100,300,400,500,700|Material+Icons">
<link href="//maxcdn.bootstrapcdn.com/font-awesome/latest/css/font-awesome.min.css" rel="stylesheet">
<link href="//cdnjs.cloudflare.com/ajax/libs/academicons/1.9.2/css/academicons.min.css" rel="stylesheet" >
<link href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/6.4.0/css/all.min.css" rel="stylesheet">

<!-- Code Syntax Highlighting -->
<!-- <link rel="stylesheet" href="https://gitcdn.link/repo/jwarby/jekyll-pygments-themes/master/github.css" /> -->

<!-- Styles -->
<!-- 
<link rel="icon" href="data:image/svg+xml,<svg xmlns=%22http://www.w3.org/2000/svg%22 viewBox=%220 0 100 100%22><text y=%22.9em%22 font-size=%2290%22></text></svg>">
i
 -->

<link rel="icon" type="image/png" href="/collections/photo/group/lab_logo.png">

<link rel="stylesheet" href="/assets/css/main.css">
<link rel="canonical" href="/news/">

<!-- JQuery -->
<!-- jQuery -->
<script src="https://cdnjs.cloudflare.com/ajax/libs/jquery/3.5.1/jquery.min.js" integrity="sha512-bLT0Qm9VnAYZDflyKcBaQ2gg0hSYNQrJ8RilYldYQ1FxQYoCLtUjuuRuZo+fjqhx/qtq/1itJ0C2ejDxltZVFg==" crossorigin="anonymous"></script>


<!-- Theming-->






    
<!-- MathJax -->
<script type="text/javascript">
  window.MathJax = {
    tex: {
      tags: 'ams'
    }
  };
</script>
<script defer type="text/javascript" id="MathJax-script" src="https://cdn.jsdelivr.net/npm/mathjax@3.2.0/es5/tex-mml-chtml.js"></script>
<script defer src="https://polyfill.io/v3/polyfill.min.js?features=es6"></script>


  </head>

  <body class="fixed-top-nav ">

    <!-- Header -->

    

<header>


  <!-- Nav Bar -->
  <nav id="navbar" class="navbar navbar-light navbar-expand-sm fixed-top">


    <!-- <nav id="navbar" class="navbar navbar-light navbar-expand-sm "> -->



      <div class="container">


        <a class="navbar-brand" href="../">
          <img id="logo-main" base src="/assets/img/lab_logo.png" width="250" alt="Logo Thing main logo"><br>
        </a>


        <!-- Navbar Toggle -->

        <button class="navbar-toggler collapsed ml-auto" type="button" data-toggle="collapse" data-target="#navbarNav" aria-controls="navbarNav" aria-expanded="false" aria-label="Toggle navigation">
          <span class="sr-only">Toggle navigation</span>
          <span class="icon-bar top-bar"></span>
          <span class="icon-bar middle-bar"></span>
          <span class="icon-bar bottom-bar"></span>
        </button>


        <div class="collapse navbar-collapse text-right" id="navbarNav">
          <ul class="navbar-nav mx-auto flex-nowrap">




            <!-- About -->
<!--             <li class="nav-item ">
              <a class="nav-link" href="/">
                about
                
              </a>
            </li> -->

            <!-- Other pages -->
            
            
            
            
            
            
            <li class="nav-item ">
              <a class="nav-link" href="/research/">
                research
                
              </a>
            </li>
            
            
            
            
            
            <li class="nav-item ">
              <a class="nav-link" href="/publications/">
                publications
                
              </a>
            </li>
            
            
            
            
            
            <li class="nav-item ">
              <a class="nav-link" href="/people/">
                people
                
              </a>
            </li>
            
            
            
            
            
            <li class="nav-item ">
              <a class="nav-link" href="/teaching/">
                teaching
                
              </a>
            </li>
            
            
            
            
            
            <li class="nav-item ">
              <a class="nav-link" href="/robots/">
                lab
                
              </a>
            </li>
            
            
            
            
            
            <li class="nav-item ">
              <a class="nav-link" href="/joining/">
                joining
                
              </a>
            </li>
            
            
            
            
            
            
            
            
            
            
            
            <li class="nav-item active">
              <a class="nav-link" href="/news/">
                news
                
                <span class="sr-only">(current)</span>
                
              </a>
            </li>
            
            
            
            
            
            
            
            
            
            
            
            
            
            
            
            
          </ul>
        </div>
      </div>
    </nav>

  </header>




    <!-- Content -->

    <div class="container mt-5">
      <article>
  <h3 id="news-archive"><strong>News Archive</strong></h3>

<p style="margin-bottom:0.6cm"> </p>

<div class="updates-list">

<div class="update-item">
    <div class="update-date">Jun. 2, 2025</div>
    <div class="update-content">
      <!-- <a  href="https://x.com/jinwanxin/status/1828130385806651428" target="_blank" role="button"> <i class="fa-brands fa-twitter fa-shake"></i> ğ•-Twitter </a> -->
      <p>
        ğŸš€ğŸš€ Tracking unknown, highly dynamic objects in <strong>contact-rich scenes</strong>  â€” falling, bouncing, in-hand manipulation? Occlusion, blur, and impacts make it tough. Introducing ğŸ”¥ğŸ”¥<a href="https://irislab.tech/TwinTrack-webpage/" target="_blank"><u>TwinTrack</u></a>ğŸ”¥ğŸ”¥: a physics-aware, <strong><u>real-time</u></strong> (&gt;20â€¯Hz) tracker that fuses <strong><u>Real2Sim</u></strong>  + <strong><u>Sim2Real</u></strong>, bridging vision and contact physics for reliable tracking. All GPU-accelerated!
      </p>
      <div class="video-container" style="position: relative; width: 100%; padding-top: 19%;">
        <video autoplay="" loop="" muted="" controls="" style="position: absolute; top: 0; left: 0; width: 100%; height: 100%;">
          <source src="/collections/research/manipulation/TwinTrack/video.mp4" type="video/mp4" />
        </video>
      </div>
      <br />
      <ul style="font-size: 1rem; line-height: 1.6; padding-left:0">
        <li>ğŸ§  <strong>Real2Sim</strong>: infers object geometry &amp; physical properties from vision + contact cues</li>
        <li>ğŸ‘ï¸ <strong>Sim2Real</strong>: predicts motion via contact dynamics, fused with visual tracking</li>
        <li>ğŸ’ª Robust to occlusion, blur, and impacts â€” no prior object model required</li>
        <li>ğŸš€ GPU-accelerated and runs in real time at &gt;20â€¯Hz</li>
      </ul>
        Check out the <a href="https://irislab.tech/TwinTrack-webpage/" target="_blank"> webpage</a>,  <a href="https://arxiv.org/abs/2505.22882" target="_blank">preprint</a>, and <a href="https://youtu.be/UGn-kZzQsEg?si=mB18q3xX_reGZdQZ" target="_blank">method video</a>.
      <br /> <br />

</div>
</div>

<br /><br /><br />

<div class="update-item">
    <div class="update-date">May 1, 2025</div>
      <div class="update-content">
      <p>
         ãŠ—ï¸ ğŸ‰: Our paper <a href="https://arxiv.org/abs/2502.02921" target="_blank">â€œRobust Reward Alignment via Hypothesis Space Batch Cuttingâ€</a> has been accepted to International Conference on Machine Learning (ICML) 2025. Congrats to <a href="https://zhi-xian-xie.github.io/" target="_blank">Zhixian Xie,</a>  Haode Zhang, and Yizhe Feng!
      </p> 
        Demo, webiste, and twitter feeds will arrive soon!
      </div>
  </div>

<br /><br /><br />

<div class="update-item">
    <div class="update-date">Apr. 11, 2025</div>
      <div class="update-content">
      <p>
         ãŠ—ï¸ ğŸ‰: Our paper <a href="https://arxiv.org/pdf/2408.07855" target="_blank">â€œComplementarity-Free Multi-Contact Modeling and Optimization for Dexterous Manipulationâ€</a> has been accepted to Robotics: Science and Systems (RSS) 2025. Congrats!
      </p> 
        See our previous  
        <a href="https://x.com/jinwanxin/status/1825958958382854247" target="_blank" role="button"> <i class="fa-brands fa-twitter fa-shake"></i> ğ•-Twitter </a> and 
        <a href="https://www.youtube.com/watch?v=NsL4hbSXvFg" target="_blank" role="button"> <i class="fa-brands fa-youtube fa-shake"></i> YouTube </a> for an extended demo.
      </div>
  </div>

<br /><br /><br />

<div class="update-item">
    <div class="update-date">Mar. 20, 2025</div>
      <div class="update-content">
      <p>
         ğŸ“¢: Wanxin gave a talk at Penn State: <b>Toward Efficient &amp; Real-time Robotic Dexterity:
         A model-based approach for contact-rich control, learning, &amp; perception</b>. Talk slides will be released soon!
      </p> 
      </div>
  </div>

<br /><br /><br />

<div class="update-item">
    <div class="update-date">Feb. 16, 2025</div>
      <div class="update-content">
      <p>
         ãŠ—ï¸ ğŸ‰: Wen's paper <a href="https://arxiv.org/abs/2408.09612" target="_blank">â€œContactSDF: Signed Distance Functions as Multi-Contact Models for Dexterous Manipulationâ€</a> has been accepted to RA-L. Congrats to Wen!
      </p> 
        See our previous  
        <a href="https://x.com/jinwanxin/status/1828130385806651428" target="_blank" role="button"> <i class="fa-brands fa-twitter fa-shake"></i> ğ•-Twitter </a> and 
        <a href="https://youtu.be/2AsMYCT-jQI?si=yPK3kec3lj-85kts" target="_blank" role="button"> <i class="fa-brands fa-youtube fa-shake"></i> YouTube </a> for a brief introduction.

</div>
</div>

<br /><br /><br />

<div class="update-item">
    <div class="update-date">Oct 15, 2024</div>
    <div class="update-content">
      <a href="https://x.com/adcock_brett/status/1848032460023468508" target="_blank" role="button"> <i class="fa-brands fa-twitter fa-shake"></i> ğ•-Twitter </a>
      <p>
      ğŸ”¥ğŸ”¥ <strong>â€œSkills from YouTube, No Prep!â€</strong> ğŸ”¥ğŸ”¥
      Can robots learn skills from YouTube without complex video processing?
      Our <a href="https://arxiv.org/abs/2410.09286" target="_blank">"Language-Model-Driven Bi-level Methodâ€</a>  makes it possible! By chaining VLM &amp; LLM in a bi-level framework, we use the â€œchain ruleâ€ to guide reward learning directly from video demos. ğŸš€Check out our RL agents mastering skills from their biological counterparts!ğŸš€
      </p>
      <div class="video-container">
        <video autoplay="" loop="" muted="" controls="">
          <source src="/collections/research/human/lfd-llm/lfd-llm.mp4" type="video/mp4" />
        </video>
      </div>
      <br />
        Check out the <a href="https://arxiv.org/abs/2410.09286" target="_blank">preprint</a>. Here is a long demo:
      <br /> <br /> 
      <div class="video-container">
        <iframe width="560" height="315" src="https://www.youtube.com/embed/CzlyYLu4mLQ?si=jyh1nZdADkGYpAAQ" title="YouTube video player" frameborder="0" allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture; web-share" referrerpolicy="strict-origin-when-cross-origin" allowfullscreen=""></iframe>
      </div>
    </div>
  </div>

<br /><br /><br />

<div class="update-item">
    <div class="update-date">Aug 24, 2024</div>
    <div class="update-content">
      <a href="https://x.com/jinwanxin/status/1828130385806651428" target="_blank" role="button"> <i class="fa-brands fa-twitter fa-shake"></i> ğ•-Twitter </a>
      <p>
        ğŸš€ Can a robotic hand master dexterous manipulation  <strong>in just 2 minutes</strong>? YES! ğŸ‰ Excited to share our recent work  <a href="https://arxiv.org/abs/2408.09612" target="_blank">â€œContactSDFâ€</a>, a physics-inspired representation using signed distance functions (SDFs) for contact-rich manipulation, from geometry to MPC. ğŸ”¥ Watch <strong>a full, uncut video</strong> of Allegro hand learning from scratch below!   We are pushing the boundaries of â€œfastâ€ learning and planning in dexterous manipulation.
      </p>
      <div class="video-container">
        <video autoplay="" loop="" muted="" controls="">
          <source src="/collections/research/manipulation/contactsdf/contactSDF-media.mp4" type="video/mp4" />
        </video>
      </div>
      <br />
        Check out the <a href="https://yangwen-1102.github.io/contactsdf.github.io/" target="_blank"> webpage</a>,  <a href="https://arxiv.org/abs/2408.09612" target="_blank">preprint</a>, and <a href="https://github.com/asu-iris/ContactSDF" target="_blank">code</a>. Here is a long demo:
      <br /> <br /> 
      <div class="video-container">
        <iframe src="https://www.youtube.com/embed/2AsMYCT-jQI?si=_7Y9LgnSfuF8yIoL" title="YouTube video player" frameborder="0" allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture; web-share" referrerpolicy="strict-origin-when-cross-origin" allowfullscreen=""></iframe>
      </div>
    </div>
  </div>

<br /><br /><br />

<div class="update-item">
    <div class="update-date">Aug 19, 2024</div>
    <div class="update-content">
      <a href="https://x.com/jinwanxin/status/1825958958382854247" target="_blank" role="button"> <i class="fa-brands fa-twitter fa-shake"></i> ğ•-Twitter </a>
      <p>
      Can model-based planning and control rival or even surpass reinforcement learning in challenging dexterous manipulation tasks? Our answer is a resounding YES! PROUD to share: ğŸ”¥ğŸ”¥<a href="https://arxiv.org/abs/2408.07855" target="_blank">"Complementarity-Free Multi-Contact Modeling and Optimization,"</a>,  our latest method that sets shattering benchmarks in various challenging dexterous manipulation tasks.
      </p>
      <p> <a href="https://arxiv.org/abs/2408.07855" target="_blank">"Complementarity-Free Multi-Contact Modeling and Optimization,"</a>  consistently achieves state-of-the-art results across different challenging dexterous manipulation tasks, including fingertip 3D in-air manipulation, TriFinger in-hand manipulation, and Allegro hand on-palm manipulation, all with different objects. Check out the demo below!
      </p>
      Our method sets a new benchmark in dexterous manipulation:
      <ul>
        <li>ğŸ¯ A 96.5% success rate across all tasks</li>
        <li>âš™ï¸ High manipulation accuracy: 11Â° reorientation error &amp; 7.8 mm position error</li>
        <li>ğŸš€ Model predictive control running at 50-100 Hz for all tasks</li>
      </ul>
      <div class="video-container">
        <video autoplay="" loop="" muted="" controls="">
          <source src="/collections/research/manipulation/teaser-grid.mp4" type="video/mp4" />
        </video>
      </div>
      <br />
        Check out our <a href="https://arxiv.org/abs/2408.07855" target="_blank"> preprint</a>, and try out our  <a href="https://github.com/asu-iris/Complementarity-Free-Dexterous-Manipulation" target="_blank">code</a> (fun guaranteed). Here is a long demo:
      <br /> <br /> 
      <div class="video-container">
        <iframe src="https://www.youtube.com/embed/NsL4hbSXvFg?si=eICS9JW-ZxMTxOtm" title="YouTube video player" frameborder="0" allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture; web-share" referrerpolicy="strict-origin-when-cross-origin" allowfullscreen=""></iframe>
      </div>
    </div>
  </div>

<br /><br /><br />

<div class="update-item">
    <div class="update-date">July 9 2024</div>
    <div class="update-content">
      <a href="https://x.com/jinwanxin/status/1815216477748019473" target="_blank" role="button"> <i class="fa-brands fa-twitter fa-shake"></i> ğ•-Twitter </a>
      <p>
        ğŸ¤– Robots may be good at inferring a task reward from human feedback, but how about inferring safety boundaries from human feedback? In many cases such as robot feeding and liquid pouring, specifying  user-comfortable safety constraints is more challenging  than  rewards. Our recent work, led by my PhD student <a href="https://zhi-xian-xie.github.io/" target="_blank">Zhixian Xie</a>,  shows that this is possible, and can actually be very human-effort efficient! Our method is called <a href="https://arxiv.org/abs/2407.04216" target="_blank">Safe MPC Alignment</a> (submitted to T-RO), enabling a robot to learn its control safety constraints with only a small handful of human online corrections!
      </p>
      Importantly, the Safe MPC Alignment is certifiable: providing an upper bound on the total number of human feedback in the case of successful learning of safety constraints, or declaring the misspecification of the hypothesis space, i.e., the true implicit safety constraint cannot be found within the specified hypothesis space. 
      <div class="video-container">
        <video autoplay="" loop="" muted="" controls="">
          <source src="/collections/research/human/media-robot.mp4" type="video/mp4" />
        </video>
      </div>
      <br />
      Check out the <a href="https://zhi-xian-xie.github.io/safe_alignment_site/" target="_blank">project website</a>, <a href="https://arxiv.org/abs/2407.04216" target="_blank"> preprint</a>, <a href="https://ieeexplore.ieee.org/document/11342351" target="_blank">paper</a>, and a breaf introduction vide below.
      <br /> 
      <div class="video-container">
        <iframe src="https://www.youtube.com/embed/QOODShHLQJE?si=IuYvkp3wm507Dc2Y" title="YouTube video player" frameborder="0" allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture; web-share" referrerpolicy="strict-origin-when-cross-origin" allowfullscreen=""></iframe>
      </div>
    </div>
  </div>

<!-- Add more update items as needed -->

</div>

</article>



    </div>

    <!-- Footer -->

    
<footer class="fixed-bottom">
  <div class="container mt-0">
    &copy; Copyright 2026   IRIS Lab.
    
    
    
    Last updated: January 15, 2026.
    
  </div>
</footer>



  </body>

  <!-- Bootsrap & MDB scripts -->
<script src="https://cdnjs.cloudflare.com/ajax/libs/popper.js/2.4.4/umd/popper.min.js" integrity="sha512-eUQ9hGdLjBjY3F41CScH3UX+4JDSI9zXeroz7hJ+RteoCaY+GP/LDoM8AO+Pt+DRFw3nXqsjh9Zsts8hnYv8/A==" crossorigin="anonymous"></script>
<script src="https://stackpath.bootstrapcdn.com/bootstrap/4.5.2/js/bootstrap.min.js" integrity="sha512-M5KW3ztuIICmVIhjSqXe01oV2bpe248gOxqmlcYrEzAvws7Pw3z6BK0iGbrwvdrUQUhi3eXgtxp5I8PDo9YfjQ==" crossorigin="anonymous"></script>
<script src="https://cdnjs.cloudflare.com/ajax/libs/mdbootstrap/4.19.1/js/mdb.min.js" integrity="sha512-Mug9KHKmroQFMLm93zGrjhibM2z2Obg9l6qFG2qKjXEXkMp/VDkI4uju9m4QKPjWSwQ6O2qzZEnJDEeCw0Blcw==" crossorigin="anonymous"></script>

  
<!-- Mansory & imagesLoaded -->
<script defer src="https://unpkg.com/masonry-layout@4/dist/masonry.pkgd.min.js"></script>
<script defer src="https://unpkg.com/imagesloaded@4/imagesloaded.pkgd.min.js"></script>
<script defer src="/assets/js/mansory.js" type="text/javascript"></script>


  



<!-- Load Common JS -->
<script src="/assets/js/common.js"></script>


</html>
